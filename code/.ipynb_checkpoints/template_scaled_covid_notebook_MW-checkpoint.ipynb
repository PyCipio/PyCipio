{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import stuff ###### \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from covid19dh import covid19\n",
    "from datetime import date\n",
    "from Get_covid_data import get_data\n",
    "import pymc3 as pm\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import theano\n",
    "import theano.tensor as tt \n",
    "import random\n",
    "import fns as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Part 1: the data we receive #####\n",
    "df = get_data(level = 2, start = date(2020,3,6)) #can get more or less data here.\n",
    "df = df[df[\"administrative_area_level_2\"].isin([\"Colorado\", \"Mississippi\", \"New York\", \"Texas\"])]\n",
    "df[\"t\"] = df[\"date\"]\n",
    "df[\"y\"] = df[\"new_infected_pr_capita\"]\n",
    "df[\"idx\"] = df[\"administrative_area_level_2\"]\n",
    "df = df[[\"idx\", \"t\", \"y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## user also gives us names \n",
    "time = \"t\"\n",
    "values = \"y\"\n",
    "index = \"idx\"\n",
    "split = .20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Part 2: our preprocessing ######\n",
    "index_codes = \"idx_code\"\n",
    "time_codes = \"t_code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## handling idx if missing\n",
    "if index == \"None\": \n",
    "    df[index] == np.zeros(len(df))\n",
    "    df[index_codes] == np.zeros(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create idx codes\n",
    "df[index_codes] = pd.Categorical(df[index]).codes\n",
    "\n",
    "## handling time \n",
    "if type(df[time]) != int: \n",
    "    df[time_codes] = df.groupby([index]).cumcount()+0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train / test split \n",
    "train, test = f.train_test(df, time, train_size = split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## variables for test \n",
    "t1_test = ((test[time_codes] - df[time_codes].min()) / (df[time_codes].max() - df[time_codes].min())).values\n",
    "t2_test = np.unique(t1_test)\n",
    "t3_test = len(t2_test)\n",
    "y_test = ((test[values] - df[values].min()) / (df[values].max() - df[values].min())).values\n",
    "idx_test = test[index_codes].values\n",
    "n_test = len(np.unique(idx_test))\n",
    "\n",
    "## variables for train\n",
    "t1_train = ((train[time_codes] - train[time_codes].min()) / (train[time_codes].max() - train[time_codes].min())).values\n",
    "t2_train = np.unique(t1_train) \n",
    "t3_train = len(t2_train)\n",
    "y_train = ((train[values] - train[values].min()) / (train[values].max() - train[values].min())).values\n",
    "idx_train = train[index_codes].values\n",
    "n_train = len(np.unique(idx_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Part 3: set up periods #######\n",
    "\n",
    "## NB: we assume that input is in days. \n",
    "\n",
    "## common across week & month (I guess)\n",
    "## NB: deviation might as well just go in on each place then.\n",
    "divisor = 7\n",
    "deviation = 0.2\n",
    "\n",
    "## week \n",
    "n_week_components = 4\n",
    "week_mu = 7\n",
    "week_sd = week_mu/divisor\n",
    "\n",
    "## normalize week.\n",
    "p_week_mu = (week_mu - train[time_codes].min()) / (train[time_codes].max() - train[time_codes].min())\n",
    "p_week_sd = (week_sd - train[time_codes].min()) / (train[time_codes].max() - train[time_codes].min())\n",
    "beta_week_sd = deviation\n",
    "p_week_mu\n",
    "\n",
    "## month\n",
    "n_month_components = 4\n",
    "month_mu = 30\n",
    "month_sd = month_mu/divisor\n",
    "\n",
    "## normalize month.\n",
    "p_month_mu = (month_mu -  train[time_codes].min()) / (train[time_codes].max() - train[time_codes].min())\n",
    "p_month_sd = (month_sd - train[time_codes].min()) / (train[time_codes].max() - train[time_codes].min())\n",
    "beta_month_sd = deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Part 4: Seasonal component #####\n",
    "def seasonal_component(\n",
    "    name, \n",
    "    name_beta, \n",
    "    mu, \n",
    "    sd, \n",
    "    beta_sd, \n",
    "    n_components, \n",
    "    shape,\n",
    "    t2,\n",
    "    t3):\n",
    "    \n",
    "    p = pm.Beta(name, \n",
    "                mu = mu, \n",
    "                sd = sd, \n",
    "                shape = shape)\n",
    "\n",
    "    period_x = 2*np.pi*np.arange(1, n_components+1)\n",
    "    period_stack_x = np.stack([period_x for i in range(shape)])\n",
    "    period_scaled_x = period_stack_x.T / p\n",
    "    x = tt.reshape(period_scaled_x[:, :, None] * t2, (n_components, shape*t3))\n",
    "    x_waves = tt.concatenate((tt.cos(x), tt.sin(x)), axis = 0)\n",
    "\n",
    "    beta_waves = pm.Normal(\n",
    "        name_beta, \n",
    "        mu = 0,\n",
    "        sd = beta_sd, \n",
    "        shape = (2*n_components, shape))\n",
    "\n",
    "    ### flatten waves\n",
    "    lst = []\n",
    "    index_first = 0\n",
    "    index_second = t3\n",
    "    for i in range(shape): \n",
    "        tmp = pm.math.dot(x_waves.T[index_first:index_second, :], beta_waves[:, i])\n",
    "        lst.append(tmp)\n",
    "        index_first += t3\n",
    "        index_second += t3\n",
    "    stacked = tt.stack(lst)\n",
    "    x_flat = tt.flatten(stacked)\n",
    "    \n",
    "    return (beta_waves, x_waves, x_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Part 5: run the model ######\n",
    "with pm.Model() as m0: \n",
    "    \n",
    "    # shared \n",
    "    t1_shared = pm.Data('t1_shared', t1_train)\n",
    "    t2_shared = pm.Data('t2_shared', t2_train)\n",
    "    t3_shared = pm.Data('t3_shared', np.array(t3_train))\n",
    "    idx_shared = pm.Data('idx_shared', idx_train)\n",
    "    \n",
    "    # prepare fourier week\n",
    "    #seasonal_component(name, name_beta, mu, sd, beta_sd, n_components, shape, time_scaled)\n",
    "    beta_week_waves, x_week_waves, week_flat = seasonal_component(name = \"p_week\",\n",
    "                                                       name_beta = \"beta_week_waves\",\n",
    "                                                       mu = p_week_mu,\n",
    "                                                       sd = p_week_sd,\n",
    "                                                       beta_sd = beta_week_sd,\n",
    "                                                       n_components = n_week_components,\n",
    "                                                       shape = n_train,\n",
    "                                                       t2 = t2_shared,\n",
    "                                                       t3 = t3_shared)\n",
    "    \n",
    "    beta_month_waves, x_month_waves, month_flat = seasonal_component(name = \"p_month\",\n",
    "                                                       name_beta = \"beta_month_waves\",\n",
    "                                                       mu = p_month_mu,\n",
    "                                                       sd = p_month_sd,\n",
    "                                                       beta_sd = beta_month_sd,\n",
    "                                                       n_components = n_month_components,\n",
    "                                                       shape = n_train,\n",
    "                                                       t2 = t2_shared,\n",
    "                                                       t3 = t3_shared)\n",
    "    \n",
    "    # other priors\n",
    "    beta_line = pm.Normal('beta_line', mu = 0, sd = 0.3, shape = n_train)\n",
    "    alpha = pm.Normal('alpha', mu = 0.5, sd = 0.3, shape = n_train)\n",
    "\n",
    "    mu = alpha[idx_shared] + beta_line[idx_shared] * t1_shared + week_flat * t1_shared + month_flat * t1_shared\n",
    "    \n",
    "    # sigma \n",
    "    sigma = pm.Exponential('sigma', 1)\n",
    "    \n",
    "    # likelihood \n",
    "    y_pred = pm.Normal(\n",
    "        'y_pred', \n",
    "        mu = mu,\n",
    "        sd = sigma,\n",
    "        observed = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Part 6: Sampling ######\n",
    "\n",
    "## sample prior\n",
    "with m0:\n",
    "    prior_pred = pm.sample_prior_predictive(100) # like setting this low. \n",
    "    m0_idata = az.from_pymc3(prior=prior_pred)\n",
    "\n",
    "az.plot_ppc(m0_idata, group=\"prior\")\n",
    "\n",
    "## convenience function \n",
    "def sample_mod(\n",
    "    model, \n",
    "    posterior_draws = 1000, \n",
    "    post_pred_draws = 1000,\n",
    "    prior_pred_draws = 500):\n",
    "    \n",
    "    with model: \n",
    "        trace = pm.sample(\n",
    "            return_inferencedata = False, \n",
    "            draws = posterior_draws,\n",
    "            target_accept = .95) # tuning!\n",
    "        post_pred = pm.sample_posterior_predictive(trace, samples = post_pred_draws)\n",
    "        prior_pred = pm.sample_prior_predictive(samples = prior_pred_draws)\n",
    "        m_idata = az.from_pymc3(trace = trace, posterior_predictive=post_pred, prior=prior_pred)\n",
    "    \n",
    "    return m_idata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample posterior & predictive\n",
    "m0_idata = sample_mod(m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot checks \n",
    "az.plot_ppc(m0_idata, num_pp_samples = 100, group = \"prior\")\n",
    "az.plot_ppc(m0_idata, num_pp_samples = 100)\n",
    "\n",
    "## plot trace\n",
    "az.plot_trace(m0_idata)\n",
    "\n",
    "###### Part 7: check fit to data #######\n",
    "m_pred = m0_idata.posterior_predictive.mean(axis = 1)\n",
    "m_pred_std = m_pred.std(axis = 0)\n",
    "m_pred = m_pred.mean(axis = 0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
